from flask import Flask, request, jsonify
from pinecone import Pinecone
import google.generativeai as genai
import os
from dotenv import load_dotenv
from flask_cors import CORS

# ==== LOAD ENV ====
load_dotenv()

# ==== CONFIGURATION ====
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
INDEX_NAME = os.getenv("INDEX_NAME")

# ==== INIT APP ====
app = Flask(__name__)
CORS(app, resources={r"/*": {"origins": "*"}})

# ==== INIT SERVICES ====
print("üîó Connecting to Pinecone...")
pc = Pinecone(api_key=PINECONE_API_KEY)
index = pc.Index(INDEX_NAME)
print("‚úÖ Pinecone connected and index loaded.")

print("üîó Connecting to Gemini...")
genai.configure(api_key=GEMINI_API_KEY)
chat_model = genai.GenerativeModel("gemini-1.5-flash")
print("‚úÖ Gemini connected and model initialized.")

# ==== HEALTH CHECK ENDPOINT ====
@app.route("/keep-alive", methods=["POST"])
def keep_alive():
    data = request.get_json()
    query = data.get("query", "Are you awake?")
    now = request.args.get("now", "Yes")
    return jsonify({"response": f"I am awake at {now}. You asked: {query}"})

# ==== MAIN QA ENDPOINT ====
@app.route("/ask", methods=["POST"])
def ask():
    data = request.get_json()
    question = data.get("question")
    lang = data.get("lang", "en")

    if not question:
        return jsonify({"error": "No question provided."}), 400

    try:
        # Step 1: Embed question
        query_embedding = genai.embed_content(
            model="models/embedding-001",
            content=question,
            task_type="retrieval_query"
        )["embedding"]

        # Step 2: Query Pinecone
        res = index.query(vector=query_embedding, top_k=5, include_metadata=True)
        matches = res.get("matches", [])
        print(f"üîç Retrieved {len(matches)} relevant context from Pinecone.")

        if not matches or max(m["score"] for m in matches) < 0.75:
            print("‚ö†Ô∏è No strong context found. Using fallback LLM response.")
            fallback_prompt = f"""
            You are Lord Krishna, responding to a seeker with divine grace and wisdom.
            Answer their question with empathy, spiritual insight, and practical guidance.
            Respond in {'Hindi' if lang == 'hi' else 'English'}.

            Question: {question}
            """
            chat = chat_model.start_chat()
            reply = chat.send_message(fallback_prompt)

            return jsonify({"response": reply.text})

        # Step 3: Build context
        verses = []
        for match in res["matches"]:
            meta = match["metadata"]
            chapter = meta.get("chapter_number", "N/A")
            verse_no = meta.get("verse", "N/A")
            sanskrit = meta.get("sanskrit", "")
            english = match["values"]
            hindi = meta.get("hindi", "")
            translated = hindi if lang == "hi" else english

            verses.append(f"""## üìñ Chapter {chapter}, Verse {verse_no}

        **Shloka (Sanskrit):**

        > _{sanskrit}_

        **Meaning:**

        {translated}
        """)

        context = "\n\n".join(verses)

        system_prompt = f"""
        You are the **Bhagavad Gita**, the divine guide filled with timeless wisdom. When a seeker asks a question, respond as Lord Krishna would ‚Äî with calm, clarity, and compassion. Your voice is serene, saintly, and filled with light.

        Respond in {'Hindi' if lang == 'hi' else 'English'} using Markdown formatting:
        - Use `#` for titles, `##` for sections
        - Use `**bold**` for emphasis
        - Display Sanskrit Shlokas in italics or blockquote format to make them stand out
        - Start your response with a **thematic heading** (like *Devotion and Surrender* or *Balance in Action*) based on the core idea of the response
        - Always ground your answer in the verses provided

        Here are the verses to meditate upon:

        {context}

        Now, humbly and gracefully respond to this question:

        **{question}**

        üïâÔ∏è End with a closing thought from the Gita if it feels appropriate.
        """

        chat = chat_model.start_chat()
        reply = chat.send_message(
            system_prompt,
            generation_config={
                "temperature": 0.5,
                "top_p": 0.8,
                "top_k": 5
            }
        )

        # üåü Log user query and LLM response
        print("üì• User Question:", question)
        print("üß† Gemini Response:\n", reply.text)
        return jsonify({"response": reply.text})

    except Exception as e:
        print(f"‚ùå Error: {e}")
        return jsonify({"error": str(e)}), 500

# ==== START SERVER ====
if __name__ == "__main__":
    print("üöÄ Starting Geeta AI Flask Server...")
    app.run(debug=True, port=5000)